---
title: "Choosing a Summarization strategy for Data-driven analysis of isobarically labeled proteomic data."
author: "Joris Van Houtven, Piotr Prostko"
date: '`r format(Sys.time(), "%B %d, %Y,%H:%M")`'
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
    theme: flatly
    code_folding: "hide"
editor_options: 
  chunk_output_type: console
---
  
```{r, setup, include=FALSE}
# default knitr options for all chunks
knitr::opts_chunk$set(
  message=FALSE,
  warning=FALSE,
  fig.width=12,
  fig.height=7
)
```

```{r}
library(ggplot2)
library(stringi)
library(gridExtra)
library(dendextend)
library(kableExtra)
library(psych)
library(limma)
library(tidyverse)
library(preprocessCore)
library(CONSTANd)  # install from source: https://github.com/PDiracDelta/CONSTANd/
library(NOMAD)  # devtools::install_github("carlmurie/NOMAD")
library(vsn)
```

This notebook presents isobaric labeling data analysis strategy that includes data-driven normalization. 

We will check how varying analysis components [summarization/normalization/differential abundance testing methods] changes end results of a quantitative proteomic study.

```{r}
source('./other_functions.R')
source('./plotting_functions.R')

# you should either make a symbolic link in this directory
study.design=read.delim('msstatstmt_studydesign.csv')
data.list <- readRDS('input_data.rds')
dat.l <- data.list$dat.l # data in long format
# dat.w <- data.list$dat.w # data in wide format
if ('X' %in% colnames(dat.l)) { dat.l$X <- NULL }

# remove shared peptides
shared.peptides <- dat.l %>% filter(!shared.peptide)

# keep spectra with isolation interference <30 and no missing quantification channels
dat.l <- dat.l %>% filter(isoInterOk & noNAs)

# which proteins were spiked in?
spiked.proteins <- dat.l %>% distinct(Protein) %>% filter(stri_detect(Protein, fixed='ups')) %>% pull %>% as.character

# which peptides were identified in each MS run?
unique.pep=dat.l %>% 
  group_by(Run) %>%
  distinct(Peptide) %>% 
  mutate(val=1)
unique.pep <- xtabs(val~Peptide+Run, data=unique.pep)
tmp <- apply(unique.pep, 1, function(x) all(x==1))
inner.peptides <- rownames(unique.pep)[tmp]
```

```{r}
# specify # of varying component variants and their names
variant.names <- c('quantile', 'CONSTANd', 'NOMAD', 'medianSweeping', 'CycLoessVSN')
n.comp.variants <- length(variant.names)
scale.vec <- c('log', 'log', 'log', 'log', 'log')
# pick reference channel and condition for making plots / doing DEA
quanCols <- unique(dat.l$Channel)
referenceChannel <- '127C'
referenceCondition <- '0.5'
# specify colours corresponding to biological conditions
condition.colour <- tribble(
  ~Condition, ~Colour,
  "0.125", 'black',
  "0.5", 'blue',
  "0.667", 'green',
  "1", 'red' )
# create data frame with sample info (distinct Run,Channel, Sample, Condition, Colour)
sample.info <- get_sample_info(dat.l, condition.colour)
```

# Unit component

## log2 transformation of reporter ion intensities

```{r}
dat.unit.l <- dat.l %>% mutate(response=log2(Intensity)) %>% select(-Intensity)
```

# Normalization component

```{r}
# switch to wide format
dat.unit.w <- pivot_wider(data = dat.unit.l, id_cols=-one_of(c('Condition', 'BioReplicate')), names_from=Channel, values_from=response)
# dat.unit.w2 <- lapply(dat.unit.l, function(x) {
#   pivot_wider(data = x, id_cols=-one_of(c('Condition', 'BioReplicate')), names_from=c('Run','Channel'), values_from=response)
# })
```


```{r}
dat.norm.w <- emptyList(variant.names)
```

## Quantile

We apply quantile normalization to each Run separately, and then re-scale the observations so that the mean observation within in each run is set equal to the mean observation across all runs.

```{r}
grand_average <- mean(as.matrix(dat.unit.w[,quanCols]))
x.split <- split(dat.unit.w, dat.unit.w$Run)  
x.split.norm <- lapply(x.split, function(y) {
  # apply normalizeQuantiles to each Run separately
  y[,quanCols] <- normalize.quantiles(as.matrix(y[,quanCols]))
  # make averages of all runs equal.
  y[,quanCols] <- y[,quanCols] / mean(colMeans(y[,quanCols])) * grand_average
  return(y)
})
dat.norm.w$quantile <- bind_rows(x.split.norm)
```

## CONSTANd

```{r}
# dat.unit.l entries are in long format so all have same colnames and no quanCols
x.split <- split(dat.unit.w, dat.unit.w$Run)  # apply CONSTANd to each Run separately
x.split.norm  <- lapply(x.split, function(y) {
  y[,quanCols] <- CONSTANd(y[,quanCols])$normalized_data
  return(y)
})
dat.norm.w$CONSTANd <- bind_rows(x.split.norm)
```

## NOMAD

We apply NOMAD on the PSM level instead of the peptide level.

```{r}
# doRobust=F: use means, like CONSTANd; doLog=F: values are already transformed.
dat.nomadnorm <- nomadNormalization(dat.unit.l$response, dat.unit.l %>% rename(iTRAQ=Channel) %>% as.data.frame, doRobust = FALSE, doiTRAQCorrection = FALSE, doLog = FALSE)
dat.nomadnorm$x$response <- dat.nomadnorm$y
dat.norm.w$NOMAD <- pivot_wider(data = dat.nomadnorm$x, id_cols=-one_of(c('Condition', 'BioReplicate')), names_from=iTRAQ, values_from=response)
# get rid of factors
#factornames <- names(dat.norm.w$NOMAD)[sapply(dat.norm.w$NOMAD, is.factor)]
#dat.norm.w$NOMAD <- dat.norm.w$NOMAD %>% mutate(across(factornames, remove_factors))
```

## medianSweeping

```{r}
# subtract the spectrum median log2intensity from the observed log2intensities
dat.norm.w$medianSweeping <- dat.unit.w
dat.norm.w$medianSweeping[,quanCols] <- sweep(dat.norm.w$medianSweeping[,quanCols], 1, apply(dat.norm.w$medianSweeping[,quanCols], 1, median) )
```

## Cyclic loess normalization 

```{r}
x.split <- split(dat.unit.w, dat.unit.w$Run)
x.split.norm  <- lapply(x.split, function(y) {
  y[,quanCols] <- normalizeCyclicLoess(y[,quanCols], span=0.005, iterations=15)
  return(y)
})
dat.norm.w$CycLoessVSN <- bind_rows(x.split.norm)
vsn.tmp <- vsn2(as.matrix(dat.unit.w[,quanCols]), strata=dat.unit.w$Run)
dat.norm.w$CycLoessVSN[,quanCols] <- vsn.tmp@hx
```

# Summarization component

Summarize quantification values from PSM to peptide (first step) to protein (second step).

## Median summarization (PSM to peptide to protein)

```{r}
# normalized data
dat.norm.summ.w <- lapply(dat.norm.w, function(x) {
  # group by (run,)protein,peptide then summarize twice (once on each level)
  # add select() statement because summarise_at is going bananas over character columns
  y <- x %>% group_by(Run, Protein, Peptide) %>% select(Run, Protein, Peptide, quanCols) %>% summarise_at(.vars = quanCols, .funs = median) %>% select(Run, Protein, quanCols) %>% summarise_at(.vars = quanCols, .funs = median) %>% ungroup()
  return(y)
})
```

```{r}
# make data completely wide (also across runs)
## normalized data
dat.norm.summ.w2 <- lapply(dat.norm.summ.w, function(x) {
  return(x %>% pivot_wider(names_from = Run, values_from = all_of(channelNames), names_glue = "{Run}:{.value}"))
})
colnames(dat.norm.summ.w2)
# apply normalizeQuantiles again, now to the data from all runs simultaneously
dat.norm.summ.w2$quantile[,sample.info$Sample] <- normalize.quantiles(as.matrix(dat.norm.summ.w2$quantile[,sample.info$Sample]))
dat.norm.summ.w2
```

Notice that the row sums are not equal to Ncols anymore, because the median summarization
does not preserve them (but mean summarization does).

Let's also summarize the non-normalized data for comparison in the next section.
```{r}
# non-normalized data
# add select() statement because summarise_at is going bananas over character columns
dat.nonnorm.summ.w <- dat.unit.w %>% group_by(Run, Protein, Peptide) %>% select(Run, Protein, Peptide, quanCols) %>% summarise_at(.vars = quanCols, .funs = median) %>% select(Run, Protein, quanCols) %>% summarise_at(.vars = quanCols, .funs = median) %>% ungroup()
```

```{r}
# medianSweeping: in each channel, subtract median computed across all proteins within the channel
# do the above separately for each MS run
x.split <- split(dat.norm.summ.w$medianSweeping, dat.norm.summ.w$medianSweeping$Run)  
x.split.norm  <- lapply(x.split, function(y) {
  y[,quanCols] <- sweep(y[,quanCols], 2, apply(y[,quanCols], 2, median) )
  return(y)
})
dat.norm.summ.w$medianSweeping <- bind_rows(x.split.norm)
```

# QC plots

```{r}
# make data completely wide (also across runs)

## non-normalized data
# we alreaady did this earlier, right before the second stage of quantile normalization!
#dat.nonnorm.summ.w2 <- dat.nonnorm.summ.w %>% pivot_wider(names_from = Run, values_from = all_of(quanCols), names_glue = "{Run}:{.value}")

## normalized data
dat.norm.summ.w2 <- lapply(dat.norm.summ.w, function(x) {
  dat.tmp <- x %>% pivot_wider(names_from = Run, values_from = all_of(quanCols), names_sep = ":")
  dat.tmp <- flip_colnames(dat.tmp, 'Protein')
  return(dat.tmp)
})
```

```{r}
# additional VSN on protein level data applied to all runs 
vsn.tmp <- vsn2(as.matrix(dat.norm.summ.w2$CycLoessVSN %>% select(-Protein)))
dat.norm.summ.w2$CycLoessVSN[, colnames(dat.norm.summ.w2$CycLoessVSN)!='Protein'] <- vsn.tmp@hx
```

## Boxplots

```{r}
# use (half-)wide format
par(mfrow=c(3,2))
boxplot_w(dat.nonnorm.summ.w,study.design, 'Raw')
for (i in 1: n.comp.variants){
  boxplot_w(dat.norm.summ.w[[i]], study.design, paste('Normalized', variant.names[i], sep='_'))
}
par(mfrow=c(1, 1))
```

## MA plots

MA plots of two single samples taken from condition 1 and condition 0.125, measured in different MS runs (samples *Mixture2_1:127C* and *Mixture1_2:129N*, respectively).

```{r}
par(mfrow=c(3,2))
# different unit variants require different computation of fold changes and average abuandance: additive or multiplicative scale; see maplot_ils function 
# use wide2 format
p <- vector('list', n.comp.variants+1)
p[[1]] <- maplot_ils(dat.nonnorm.summ.w2, 'Mixture2_1:127C', 'Mixture1_2:129N', scale.vec[i], 'Raw')
for (i in 1: n.comp.variants){
 p[[i+1]]<- maplot_ils(dat.norm.summ.w2[[i]], 'Mixture2_1:127C', 'Mixture1_2:129N', scale.vec[i], paste('Normalized', variant.names[i], sep='_'))
}
grid.arrange(grobs = p, ncol=2, nrow=3)
par(mfrow=c(1, 1))
```

MA plots of all samples from condition `1` and condition `0.125` (quantification values averaged within condition).

```{r}
par(mfrow=c(3,2))
# different unit variants require different computation of fold changes and average abuandance: additive or multiplicative scale; see maplot_ils function 
channels.num <- sample.info %>% filter(Condition=='1') %>% select(Sample) %>% pull
channels.denom <- sample.info %>% filter(Condition=='0.125') %>% select(Sample) %>% pull

p <- vector('list', n.comp.variants+1)
p[[1]] <- maplot_ils(dat.nonnorm.summ.w2, channels.num, channels.denom, scale.vec[i], 'Raw')

for (i in 1: n.comp.variants){
 p[[i+1]]<- maplot_ils(dat.norm.summ.w2[[i]], channels.num, channels.denom, scale.vec[i], paste('Normalized', variant.names[i], sep='_'))
}
grid.arrange(grobs = p, ncol=2, nrow=3)
par(mfrow=c(1, 1))
```

## CV (coefficient of variation) plots

```{r}
par(mfrow=c(3,2))
dat.nonnorm.summ.l <- lapply(list(dat.nonnorm.summ.w), function(x){
  x$Mixture <- unlist(lapply(stri_split(x$Run,fixed='_'), function(y) y[1]))
  x <- to_long_format(x, study.design)
})

dat.norm.summ.l <- lapply(dat.norm.summ.w, function(x){
  x$Mixture <- unlist(lapply(stri_split(x$Run,fixed='_'), function(y) y[1]))
  x <- to_long_format(x, study.design)
})

par(mfrow=c(2, 2))
  cvplot_ils(dat=dat.nonnorm.summ.l[[1]], feature.group='Protein', xaxis.group='Condition',
               title='Raw', abs=F)

for (i in 1: n.comp.variants){
    cvplot_ils(dat=dat.norm.summ.l[[i]], feature.group='Protein', xaxis.group='Condition', 
               title=paste('Normalized', variant.names[i], sep='_'), abs=F)
}
par(mfrow=c(1, 1))  
```

## PCA plots

### Using all proteins
```{r}
par(mfrow=c(3,2))
  pcaplot_ils(dat.nonnorm.summ.w2 %>% select(-'Protein'), info=sample.info, 'Raw')
  
for (i in seq_along(dat.norm.summ.w2)){
  # select only the spiked.proteins
  pcaplot_ils(dat.norm.summ.w2[[i]] %>% select(-'Protein'), info=sample.info, paste('Normalized', variant.names[i], sep='_'))
}
par(mfrow=c(1, 1))  
```

### Using spiked proteins only
```{r}
par(mfrow=c(3,2))
  pcaplot_ils(dat.nonnorm.summ.w2 %>% filter(Protein %in% spiked.proteins) %>% select(-'Protein'), info=sample.info, 'Raw')
  
for (i in seq_along(dat.norm.summ.w2)){
  # select only the spiked.proteins
  pcaplot_ils(dat.norm.summ.w2[[i]] %>% filter(Protein %in% spiked.proteins) %>% select(-'Protein'), info=sample.info, paste('Normalized', variant.names[i], sep='_'))
}
par(mfrow=c(1, 1))  
```

## HC (hierarchical clustering) plots

Only use spiked proteins

```{r}
par(mfrow=c(3,2))
  dendrogram_ils(dat.nonnorm.summ.w2 %>% filter(Protein %in% spiked.proteins) %>% select(-Protein), info=sample.info, 'Raw')

for (i in seq_along(dat.norm.summ.w2)){
    dendrogram_ils(dat.norm.summ.w2[[i]] %>% filter(Protein %in% spiked.proteins) %>% select(-Protein), info=sample.info, paste('Normalized', variant.names[i], sep='_'))
}
par(mfrow=c(1, 1))  
```

# DEA component

## Moderated t-test

TODO:
- Also try to log-transform the intensity case, to see if there are large differences in the t-test results.
  - done. remove this code?
NOTE:
- actually, lmFit (used in moderated_ttest) was built for log2-transformed data. However, supplying untransformed intensities can also work. This just means that the effects in the linear model are also additive on the untransformed scale, whereas for log-transformed data they are multiplicative on the untransformed scale. Also, there may be a bias which occurs from biased estimates of the population means in the t-tests, as mean(X) is not equal to exp(mean(log(X))).
```{r}
design.matrix <- get_design_matrix(referenceCondition, study.design)
dat.dea <- emptyList(names(dat.norm.summ.w2))
for (i in seq_along(dat.norm.summ.w2)) {
  this_scale <- scale.vec[match(names(dat.dea)[i], variant.names)]
  d <- column_to_rownames(as.data.frame(dat.norm.summ.w2[[i]]), 'Protein')
  dat.dea[[i]] <- moderated_ttest(dat=d, design.matrix, scale=this_scale)
}
# also see what the unnormalized results would look like
n.comp.variants <- n.comp.variants + 1
variant.names <- c(variant.names, 'raw')
scale.vec <- c(scale.vec, 'raw')
dat.dea$raw <- moderated_ttest(dat=column_to_rownames(dat.nonnorm.summ.w2, 'Protein'), design.matrix, scale='raw')
```

# Results comparison

## Confusion matrix

```{r, results='asis'}
cm <- conf_mat(dat.dea, 'q.mod', 0.05, spiked.proteins)
print_conf_mat(cm)
```

## Scatter plots
TO DO: Piotr: constant NOMAD i RAW q-values (approx. 1) generate error in scatterplots
```{r}
# character vectors containing logFC and p-values columns
dea.cols <- colnames(dat.dea[[1]])
logFC.cols <- dea.cols[stri_detect_fixed(dea.cols, 'logFC')]
q.cols <- dea.cols[stri_detect_fixed(dea.cols, 'q.mod')]
n.contrasts <- length(logFC.cols)

#scatterplot_ils(dat.dea, q.cols, 'p-values') # commented due to error, sd=0 for NOMAD and RAW
scatterplot_ils(dat.dea, logFC.cols, 'log2FC')
```

## Volcano plots

```{r}
for (i in 1:n.contrasts){
  volcanoplot_ils(dat.dea, i, spiked.proteins)
}
```

## Violin plots

Let's see whether the spiked protein fold changes make sense
```{r}
# plot theoretical value (horizontal lines) and violin per condition
dat.spiked.logfc <- lapply(dat.dea, function(x) x[spiked.proteins,logFC.cols])
dat.spiked.logfc.l <- lapply(dat.spiked.logfc, function(x) {
  x %>% rename_with(function(y) sapply(y, function(z) strsplit(z, '_')[[1]][2])) %>% pivot_longer(cols = everything(), names_to = 'condition', values_to = 'logFC') %>% add_column(Protein=rep(rownames(x), each=length(colnames(x)))) })
violinplot_ils(lapply(dat.spiked.logfc.l, filter, condition != referenceCondition))
```

# Conclusions

# Session information

```{r}
sessionInfo()
```